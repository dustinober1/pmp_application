# 8.2 Value Delivery and Outcomes

**ECO Task**: Evaluate and deliver project benefits and value

In the 2026 PMP context, a project is only successful if it delivers **Value**. Finishing "on time and under budget" is meaningless if the final product doesn't solve the customer's problem or drive the organization forward.

::: tip PMP exam trap
Many scenarios describe a project that met requirements but still “failed.” The hidden issue is usually **validation/adoption**: the output exists, but the outcome/benefit did not happen.
:::

---

## The Value Chain

To master value delivery, you must understand the distinction between these four stages:

<div class="chain-grid">
 <div class="chain-card">
 <div class="chain-title">1. Output</div>
 <div class="chain-tag">The Thing</div>
 <p>The tangible deliverable created (e.g., A new mobile banking app).</p>
 </div>
 <div class="chain-card">
 <div class="chain-title">2. Outcome</div>
 <div class="chain-tag">The Change</div>
 <p>The result or behavioral change (e.g., Customers can now bank from home).</p>
 </div>
 <div class="chain-card">
 <div class="chain-title">3. Benefit</div>
 <div class="chain-tag">The Metric</div>
 <p>The quantifiable gain realized (e.g., 20% reduction in physical branch costs).</p>
 </div>
 <div class="chain-card">
 <div class="chain-title">4. Value</div>
 <div class="chain-tag">The Worth</div>
 <p>The overall strategic impact (e.g., Market leadership and customer loyalty).</p>
 </div>
</div>

---

## Defining Value (So You Can Prove It Later)

Value is not a vibe—it needs a measurable definition.

### Success criteria checklist

- **Current state baseline** (what does “today” look like?)
- **Target outcomes** (behavior/state change you want)
- **Benefit metrics** (how you will measure improvement)
- **Owner** (who is accountable for realizing the benefit after delivery)
- **Measurement cadence** (when/how often value will be reviewed)

### Practical mapping (what to measure and who owns it)

| Item        | Example                                             | Primary owner               |
| ----------- | --------------------------------------------------- | --------------------------- |
| **Output**  | New customer portal                                 | Project team                |
| **Outcome** | Customers self-serve password resets                | Product owner / business    |
| **Benefit** | 30% reduction in call center volume                 | Benefits owner / operations |
| **Value**   | Lower operating cost + higher customer satisfaction | Sponsor / organization      |

::: info Leading vs lagging indicators
Outcomes and benefits are often **lagging** (they appear later). Use **leading indicators** during execution (training completion, pilot adoption rate, usage telemetry) to predict whether value will actually happen.
:::

---

## The Value Delivery System

Projects do not exist in a vacuum. They are part of a cascading flow of value:

- **Organizational Strategy**: Sets the North Star.
- **Portfolios**: Choosing the "right" projects to fit the strategy.
- **Programs**: Grouping related projects to achieve shared benefits.
- **Projects**: The engine of change that creates the outputs.
- **Operations**: The "Home" of realized value where products are used daily.

::: tip 2026 Strategy: The "Tail"
Value delivery doesn't end at "Go-Live." The PM must ensure a smooth **Transition to Operations**, ensuring the operational owners have the training and support to sustain the value long-term.
:::

---

## Value Delivery Frameworks

### 1. Kano Model (Customer Satisfaction Analysis)

The Kano Model helps prioritize features based on their impact on customer satisfaction.

<div class="kano-grid">
 <div class="kano-card">
 <div class="kano-title">Basic Needs</div>
 <p><strong>Expected features</strong> - absence causes dissatisfaction, presence is assumed (e.g., security, basic functionality).</p>
 </div>
 <div class="kano-card">
 <div class="kano-title">Performance Needs</div>
 <p><strong>Linear satisfaction</strong> - more is better (e.g., speed, reliability, capacity).</p>
 </div>
 <div class="kano-card">
 <div class="kano-title">Delighters</div>
 <p><strong>Unexpected features</strong> - presence creates excitement, absence doesn't hurt (e.g., innovative UX, surprise benefits).</p>
 </div>
</div>

::: tip PMP Application
In execution, prioritize basic needs first (prevents failure), then performance needs (competitive advantage), then delighters (differentiation). Budget cuts? Remove delighters first.
:::

### 2. Value Stream Mapping (VSM)

Visualize the flow of value from request to delivery, identifying waste and bottlenecks.

**Key VSM Components:**

- **Value-adding activities**: Steps that customers would pay for
- **Non-value-adding but necessary**: Compliance, approvals, transitions
- **Waste**: Delays, rework, waiting, unnecessary processes

**VSM Metrics:**

- **Lead Time**: Total time from request to delivery
- **Process Time**: Actual work time
- **% Complete & Accurate (%C&A)**: Work quality at each step
- **Activity Ratio**: Process time ÷ Lead time (higher is better)

**Example: Software Feature Delivery**

```
Request → Backlog (3 days) → Design (2 days) → Dev (5 days) →
Review (1 day waiting) → Test (3 days) → Deploy (1 day) = 15 days lead time
Actual work: 11 days | Waste: 4 days waiting | Activity Ratio: 73%
```

### 3. Benefit-Cost Ratio (BCR) & Return on Investment (ROI)

Quantify value to justify continued investment during execution.

**Formulas:**

- **BCR** = Present Value of Benefits ÷ Present Value of Costs
- **ROI** = (Net Benefits ÷ Costs) × 100%
- **Payback Period** = Initial Investment ÷ Annual Net Cash Flow

::: info Decision Criteria

- BCR > 1.0: Benefits exceed costs (favorable)
- ROI > 15%: Common organizational threshold for project approval
- Payback < 3 years: Typical acceptance for business cases
  :::

**Example Calculation:**

```
Project Cost: $500,000
Annual Benefits: $200,000 (cost savings + revenue increase)
Project Life: 5 years

BCR = ($200K × 5) ÷ $500K = 2.0 (favorable)
ROI = (($200K × 5) - $500K) ÷ $500K × 100% = 100%
Payback Period = $500K ÷ $200K = 2.5 years
```

---

## Benefits Realization Management (BRM)

Benefits realization is a **continuous process** that spans the entire project lifecycle and extends into operations.

### Benefits Realization Lifecycle

<div class="brm-phases">
 <div class="brm-phase">
 <div class="phase-num">1</div>
 <h4>Identify & Plan</h4>
 <p><strong>When:</strong> Initiation & Planning</p>
 <ul>
 <li>Define measurable benefits</li>
 <li>Establish baselines</li>
 <li>Identify benefit owners</li>
 <li>Create benefits register</li>
 <li>Map dependencies</li>
 </ul>
 </div>
 <div class="brm-phase">
 <div class="phase-num">2</div>
 <h4>Execute & Enable</h4>
 <p><strong>When:</strong> Execution</p>
 <ul>
 <li>Prioritize high-value work</li>
 <li>Track leading indicators</li>
 <li>Validate outcomes incrementally</li>
 <li>Prepare for adoption</li>
 <li>Build operational capability</li>
 </ul>
 </div>
 <div class="brm-phase">
 <div class="phase-num">3</div>
 <h4>Transition & Sustain</h4>
 <p><strong>When:</strong> Closing & Post-Project</p>
 <ul>
 <li>Transfer ownership to operations</li>
 <li>Ensure training/documentation</li>
 <li>Activate change management</li>
 <li>Schedule benefits reviews</li>
 <li>Monitor adoption metrics</li>
 </ul>
 </div>
 <div class="brm-phase">
 <div class="phase-num">4</div>
 <h4>Measure & Optimize</h4>
 <p><strong>When:</strong> Post-Implementation</p>
 <ul>
 <li>Compare actuals to targets</li>
 <li>Report to stakeholders</li>
 <li>Identify optimization opportunities</li>
 <li>Feed lessons into portfolio</li>
 <li>Confirm value sustainability</li>
 </ul>
 </div>
</div>

### Benefits Realization Governance

**Key Roles:**

- **Benefit Owner**: Accountable for realizing specific benefits (typically operational leaders)
- **Project Manager**: Enables delivery of outputs that create benefits
- **Sponsor**: Champions benefits realization at executive level
- **PMO/Portfolio Manager**: Tracks benefits across multiple projects

**Governance Activities:**

- **Benefits Review Board**: Regular meetings to assess benefit progress
- **Benefits Realization Reports**: Dashboards showing benefit tracking
- **Escalation Protocols**: When benefits are at risk, trigger corrective actions

### Enhanced Benefits Register Template

| Field                   | Description                                   | Example                                            |
| ----------------------- | --------------------------------------------- | -------------------------------------------------- |
| **Benefit ID**          | Unique identifier                             | BEN-001                                            |
| **Benefit Description** | Clear statement of benefit                    | Reduce customer onboarding time                    |
| **Benefit Category**    | Financial, operational, strategic, compliance | Operational efficiency                             |
| **Metric**              | How to measure                                | Average onboarding time (minutes)                  |
| **Baseline**            | Current state                                 | 45 minutes                                         |
| **Target**              | Desired state                                 | 15 minutes (67% improvement)                       |
| **Measurement Method**  | Data source and collection approach           | Analytics dashboard + support ticket tags          |
| **Benefit Owner**       | Accountable for realization                   | Customer Operations Manager                        |
| **Benefit Timeframe**   | When benefit expected to materialize          | 2 weeks, 1 month, 3 months post go-live            |
| **Dependencies**        | Required enablers                             | User training, updated SOPs, new support model     |
| **Assumptions**         | Conditions for success                        | 80% user adoption within 30 days                   |
| **Risks**               | Threats to realization                        | Low adoption, process resistance, technical issues |
| **Status**              | Current state                                 | On track / At risk / Realized                      |
| **Actual Results**      | Measured outcomes                             | 18 minutes (60% improvement - 90% of target)       |

---

## Prioritizing for Value (Agile + Hybrid)

Execution decisions should maximize outcomes, not activity.

High-yield prioritization methods you’ll see on the exam:

- **MoSCoW**: Must / Should / Could / Won’t (fast stakeholder alignment)
- **Value + risk first**: Deliver the highest value items early, especially when uncertainty is high
- **WSJF (Scaled Agile)**: Prioritize by **Cost of Delay ÷ Job Size** (useful mental model even if not named)

::: tip Exam shortcut
If the scenario describes uncertainty, prioritize work that **reduces risk and validates assumptions early** (spikes, prototypes, pilots, demos).
:::

### Weighted Shortest Job First (WSJF) - Deep Dive

**WSJF** is a SAFe prioritization technique that maximizes economic value by accounting for the cost of waiting.

**Formula:**

```
WSJF = Cost of Delay ÷ Job Duration (Size)
```

**Cost of Delay Components:**

- **User-Business Value**: How much value does this deliver to users/business?
- **Time Criticality**: How urgent is this? Are there deadlines or market windows?
- **Risk Reduction / Opportunity Enablement**: Does this reduce risk or enable future value?

**Calculation Example:**

| Feature   | User Value | Time Critical | Risk/Opp | CoD Total | Duration | WSJF                     |
| --------- | ---------- | ------------- | -------- | --------- | -------- | ------------------------ |
| Feature A | 8          | 5             | 3        | 16        | 8        | **2.0**                  |
| Feature B | 5          | 8             | 5        | 18        | 3        | **6.0** High Priority    |
| Feature C | 3          | 3             | 8        | 14        | 2        | **7.0** Highest Priority |
| Feature D | 8          | 2             | 2        | 12        | 13       | **0.9**                  |

**Interpretation**: Feature C and B have the highest WSJF (deliver value quickly relative to effort). Feature D, despite high user value, is deprioritized because of long duration.

::: info WSJF Best Practices

- Use **relative sizing** (Fibonacci: 1, 2, 3, 5, 8, 13) rather than absolute numbers
- Compare features **within the same context** (don't compare across products)
- Re-calculate periodically as time criticality changes
- Focus on **highest WSJF first** to maximize economic value over time
  :::

---

## Additional Financial Value Formulas

### Net Present Value (NPV)

**NPV** calculates the present value of future cash flows, accounting for the time value of money.

**Formula:**

```
NPV = Σ [Cash Flow / (1 + r)^t] - Initial Investment
```

Where: r = discount rate, t = time period

**Interpretation:**

- **NPV > 0**: Project adds value; accept
- **NPV = 0**: Project breaks even; consider other factors
- **NPV < 0**: Project destroys value; reject

**Example:**

```
Initial Investment: $100,000
Annual Cash Flows: $40,000 for 4 years
Discount Rate: 10%

Year 1: $40,000 / 1.10 = $36,364
Year 2: $40,000 / 1.21 = $33,058
Year 3: $40,000 / 1.331 = $30,053
Year 4: $40,000 / 1.464 = $27,322

Total PV of Cash Flows: $126,797
NPV = $126,797 - $100,000 = +$26,797 Accept
```

::: tip Exam Application
When comparing multiple projects with the same investment, choose the project with the **higher NPV**—it creates more value.
:::

### Internal Rate of Return (IRR)

**IRR** is the discount rate at which NPV equals zero. It represents the project's expected rate of return.

**Decision Rule:**

- **IRR > Required Rate of Return**: Accept the project
- **IRR < Required Rate of Return**: Reject the project

**Example:**

```
If a company's required rate of return is 12% and Project A has IRR = 18%:
→ Accept Project A (18% > 12%)

If Project B has IRR = 9%:
→ Reject Project B (9% < 12%)
```

::: warning IRR Limitations
IRR can be misleading for projects with non-conventional cash flows (multiple sign changes). When comparing mutually exclusive projects, NPV is generally more reliable.
:::

### Opportunity Cost

**Opportunity Cost** is the value of the best alternative foregone when making a decision.

**Exam Application:**

- If you have resources for only one project, the opportunity cost of choosing Project A is the value you would have gained from Project B
- Always consider what you're giving up, not just what you're getting

**Example:**

```
You can assign your top developer to Project A (expected benefit: $50K)
or Project B (expected benefit: $70K).

If you choose Project A:
- Opportunity Cost = $70K (the benefit you missed from Project B)
- Net Value = $50K - $70K = -$20K relative to best choice
```

### Sunk Cost Fallacy (Exam Trap!)

**Sunk costs** are costs already incurred that cannot be recovered. They should **NOT** influence future decisions.

**The Fallacy:** "We've already spent $500K, so we should keep going."

**Correct Thinking:** Only consider **future costs and future benefits**. Past spending is irrelevant to the decision.

**Example Scenario:**

```
Project Status:
- Spent so far: $500K (sunk cost)
- Estimated to complete: $300K more
- Expected benefit if completed: $400K
- Expected benefit if cancelled: $0

Decision Analysis:
- Continue: Spend $300K, get $400K → Net value = +$100K
- Cancel: Spend $0, get $0 → Net value = $0

Decision: Continue (based on future value, NOT sunk costs)
```

::: warning Exam Pattern
If a scenario mentions large investments already made and asks whether to continue, **ignore sunk costs**. Only compare future costs to future benefits. "We've come too far to stop now" is NEVER the right answer.
:::

---

## Verification vs. Validation (Classic PMP Trap)

- **Verify**: “Did we build it right?” (meets documented requirements/specs)
- **Validate**: “Did we build the right thing?” (meets stakeholder needs and produces outcomes)

If users are unhappy, the best answers usually involve **validating needs**, clarifying acceptance criteria, and re-aligning priorities—not arguing that the requirements were met.

---

## Adoption & Change Enablement (ADKAR)

Even perfect deliverables can fail if people don’t adopt them.

ADKAR provides a simple adoption lens:

1. **Awareness** (why the change is needed)
2. **Desire** (willingness to participate/support)
3. **Knowledge** (how to change)
4. **Ability** (capability to perform in the new way)
5. **Reinforcement** (sustain the change)

**Exam pattern**: If operations or users resist, strengthen the change enablement plan (communications, training, champions), not just the technical solution.

---

## Stakeholder Engagement During Execution

Stakeholder engagement doesn't stop after planning—it intensifies during execution to ensure value delivery remains aligned with needs.

### Engagement Strategies by Stakeholder Type

| Stakeholder Type                  | Engagement Approach                                     | Frequency                       | Primary Goal                                                  |
| --------------------------------- | ------------------------------------------------------- | ------------------------------- | ------------------------------------------------------------- |
| **Executive Sponsor**             | Executive briefings, steering committee meetings        | Bi-weekly or monthly            | Strategic alignment, issue escalation, resource decisions     |
| **Product Owner / Business Lead** | Sprint reviews, backlog refinement, acceptance testing  | Weekly or per iteration         | Validate outcomes, prioritize work, confirm value delivery    |
| **End Users / Customers**         | User testing, feedback sessions, demos, pilot programs  | Continuous throughout execution | Ensure usability, gather feedback, confirm adoption readiness |
| **Operations Team**               | Transition planning, training sessions, runbook reviews | Weekly in later phases          | Build capability, ensure operational readiness                |
| **Subject Matter Experts (SMEs)** | Design reviews, technical validation, workshops         | As needed (on-demand)           | Validate technical approach, ensure accuracy                  |
| **Regulatory / Compliance**       | Audit checkpoints, compliance reviews                   | At milestone gates              | Confirm adherence to standards, reduce compliance risk        |

### Active Stakeholder Engagement Techniques

<div class="engagement-techniques">
 <div class="technique-card">
 <h4> Show, Don't Tell</h4>
 <p><strong>Demos & Prototypes:</strong> Visual progress builds confidence faster than status reports. Show working increments, even if incomplete.</p>
 <p><strong>PMP Tip:</strong> If stakeholders are disengaged, increase demo frequency and invite them to hands-on sessions.</p>
 </div>
 <div class="technique-card">
 <h4> Feedback Loops</h4>
 <p><strong>Sprint Reviews, UAT, Pilot Programs:</strong> Create structured opportunities for stakeholders to provide input and validate value.</p>
 <p><strong>PMP Tip:</strong> Early and frequent validation reduces late-stage rework and change requests.</p>
 </div>
 <div class="technique-card">
 <h4> Transparent Communication</h4>
 <p><strong>Visible Progress & Issues:</strong> Use information radiators (burndown charts, dashboards) to make progress transparent.</p>
 <p><strong>PMP Tip:</strong> Hiding problems from stakeholders leads to trust erosion. Surface issues early with proposed solutions.</p>
 </div>
 <div class="technique-card">
 <h4> Co-Creation</h4>
 <p><strong>Collaborative Design Sessions:</strong> Involve stakeholders in solution design, not just requirements gathering.</p>
 <p><strong>PMP Tip:</strong> When stakeholders feel ownership, adoption increases dramatically.</p>
 </div>
</div>

### Managing Stakeholder Expectations During Execution

**Common Execution Challenges:**

1. **Scope Creep from "Helpful" Stakeholders**

- **Symptom**: "While you're at it, could you also add...?"
- **Response**: Acknowledge value, add to backlog, assess impact on scope/schedule/cost, get formal change approval
- **PMP Answer**: Use integrated change control—never accept informal scope additions

2. **Stakeholder Availability Issues**

- **Symptom**: Key stakeholders unavailable for reviews, decisions delayed
- **Response**: Escalate to sponsor, document impact of delays, propose empowered delegates
- **PMP Answer**: Update stakeholder engagement plan, escalate blockers, consider proxy decision-makers

3. **Conflicting Stakeholder Priorities**

- **Symptom**: Different stakeholders want different outcomes
- **Response**: Facilitate prioritization workshop, escalate to sponsor for tiebreaker, use MoSCoW or weighted scoring
- **PMP Answer**: Revisit project charter, confirm primary business objective, get sponsor alignment

4. **Negative Stakeholder Influence**

- **Symptom**: Resistant stakeholders spreading doubt or blocking progress
- **Response**: Increase engagement frequency, address concerns directly, involve champion stakeholders, escalate if needed
- **PMP Answer**: Update stakeholder engagement plan with targeted strategies for resistant stakeholders

::: tip Exam Insight: Stakeholder Resistance
If a scenario describes stakeholder resistance or lack of buy-in during execution, the best answer typically involves:

1. Increasing engagement (more demos, involvement in decisions)
2. Addressing concerns (one-on-one meetings, listening sessions)
3. Leveraging champions (involve supportive stakeholders to influence others)
4. NOT punishing or ignoring resistant stakeholders
   :::

---

## Transition to Operations (Operational Readiness)

To make the "tail" real, confirm readiness across people, process, and technology.

| Readiness area | What "ready" looks like                                           |
| -------------- | ----------------------------------------------------------------- |
| **People**     | Training complete, champions identified, support staffed          |
| **Process**    | SOPs updated, escalation path defined, SLAs agreed                |
| **Technology** | Monitoring in place, runbooks complete, access/provisioning ready |
| **Governance** | Ownership transferred (RACI), benefits reviews scheduled          |

---

## Value Measurement Techniques

Measuring value requires a balanced approach across multiple dimensions and timeframes.

### 1. Key Performance Indicators (KPIs)

KPIs are quantifiable metrics that track progress toward strategic objectives.

**Characteristics of Effective KPIs:**

- **Specific**: Clearly defined and unambiguous
- **Measurable**: Quantifiable with objective data
- **Achievable**: Realistic given constraints
- **Relevant**: Directly tied to business objectives
- **Time-bound**: Measured at defined intervals

**KPI Categories:**

| Category        | Purpose                         | Example KPIs                                                   |
| --------------- | ------------------------------- | -------------------------------------------------------------- |
| **Financial**   | Economic value delivered        | ROI, cost savings, revenue increase, cost avoidance            |
| **Operational** | Efficiency and effectiveness    | Cycle time, throughput, defect rate, utilization rate          |
| **Customer**    | Customer value and satisfaction | NPS, CSAT, retention rate, adoption rate, support tickets      |
| **Strategic**   | Alignment with strategic goals  | Market share, competitive position, innovation index           |
| **Quality**     | Deliverable quality             | Defect density, test coverage, rework rate, first-time quality |

**Example: E-commerce Platform Project KPIs**

```
Financial: 25% increase in online revenue within 6 months
Operational: Page load time < 2 seconds; 99.9% uptime
Customer: NPS > 50; Mobile app rating > 4.5 stars
Strategic: Capture 15% mobile commerce market share
Quality: < 1% critical defect rate post-launch
```

### 2. Objectives and Key Results (OKRs)

OKRs link ambitious objectives to measurable key results, promoting alignment and focus.

**Structure:**

- **Objective**: Qualitative, inspirational goal (the "what")
- **Key Results**: Quantitative measures of success (the "how we know we got there")

**Example: Product Launch OKR**

```
Objective: Become the preferred platform for small business owners

Key Results:
- KR1: Achieve 10,000 active users within 90 days of launch
- KR2: Reach 60% customer satisfaction score (CSAT > 4/5)
- KR3: Generate 500 organic referrals from existing users
- KR4: Reduce onboarding time from 30 minutes to 10 minutes
```

::: info OKRs vs KPIs

- **OKRs**: Time-boxed (quarterly), aspirational, change frequently, focus on growth
- **KPIs**: Ongoing, operational, stable over time, focus on maintaining standards
- Use both: OKRs for transformation, KPIs for sustainability
  :::

### 3. Balanced Scorecard

The Balanced Scorecard measures performance across four perspectives, preventing over-focus on any single dimension.

<div class="balanced-scorecard">
 <div class="scorecard-quadrant">
 <h4> Financial Perspective</h4>
 <p><strong>Question:</strong> How do we look to shareholders?</p>
 <p><strong>Metrics:</strong> ROI, profit margin, revenue growth, cost reduction</p>
 </div>
 <div class="scorecard-quadrant">
 <h4> Customer Perspective</h4>
 <p><strong>Question:</strong> How do customers see us?</p>
 <p><strong>Metrics:</strong> Customer satisfaction, retention, market share, brand perception</p>
 </div>
 <div class="scorecard-quadrant">
 <h4> Internal Process Perspective</h4>
 <p><strong>Question:</strong> What must we excel at?</p>
 <p><strong>Metrics:</strong> Quality, efficiency, innovation rate, process compliance</p>
 </div>
 <div class="scorecard-quadrant">
 <h4> Learning & Growth Perspective</h4>
 <p><strong>Question:</strong> Can we continue to improve and create value?</p>
 <p><strong>Metrics:</strong> Employee skills, system capabilities, organizational culture, innovation pipeline</p>
 </div>
</div>

**PMP Application**: When assessing project success, consider all four perspectives—not just schedule and budget.

### 4. Leading vs. Lagging Indicators

**Lagging Indicators** (outcomes that already happened):

- Revenue, profit, customer satisfaction scores
- Project completion, benefits realized
- **Challenge**: By the time you measure them, it's too late to influence them

**Leading Indicators** (predictors of future performance):

- User adoption trends, training completion rates, pilot feedback
- Velocity, defect trends, test coverage
- **Advantage**: Actionable—you can course-correct before outcomes crystallize

**Example: Software Adoption Project**

| Leading Indicators (During Execution)                | Lagging Indicators (Post-Implementation)               |
| ---------------------------------------------------- | ------------------------------------------------------ |
| Training completion rate (target: 90%)               | Active user adoption (target: 80% within 60 days)      |
| Pilot user satisfaction (target: 4/5)                | Customer satisfaction scores (target: NPS > 40)        |
| Help desk ticket trend (decreasing = good training)  | Support cost reduction (target: 30% decrease)          |
| User login frequency during pilot (engagement proxy) | User retention rate (target: 85% monthly active users) |

::: tip PMP Exam Strategy
If a scenario asks how to predict whether benefits will be realized, focus on **leading indicators** during execution, not waiting for lagging indicators post-implementation.
:::

### 5. Net Promoter Score (NPS) Formula

**NPS** measures customer loyalty and satisfaction, commonly used to track value delivery.

**Formula:**

```
NPS = % Promoters - % Detractors
```

**Categories (based on 0-10 scale question: "How likely are you to recommend...?"):**

- **Promoters (9-10)**: Loyal enthusiasts who drive growth
- **Passives (7-8)**: Satisfied but unenthusiastic; vulnerable to competitors
- **Detractors (0-6)**: Unhappy customers who can damage brand

**Calculation Example:**
| Response | Count | Percentage |
|---|---|---|
| Promoters (9-10) | 45 | 45% |
| Passives (7-8) | 30 | 30% |
| Detractors (0-6) | 25 | 25% |

**NPS = 45% - 25% = +20**

**Interpretation Guide:**
| NPS Range | Interpretation | Action |
|---|---|---|
| **70+** | Excellent | Maintain and leverage for growth |
| **50-69** | Strong | Continue improvements, address specific feedback |
| **30-49** | Good | Identify and address detractor concerns |
| **0-29** | Needs improvement | Investigate root causes, prioritize improvements |
| **< 0** | Critical | Immediate intervention required; major issues |

::: info NPS vs. CSAT

- **NPS**: Measures loyalty and likelihood to recommend (relationship)
- **CSAT**: Measures satisfaction with specific interaction (transactional)
- Use both: NPS for overall value delivery; CSAT for specific deliverable acceptance
  :::

---

## Quality Execution in Value Context

Quality is not just about defect detection—it's about building value into every deliverable. Quality issues during execution directly threaten value delivery.

### Quality vs. Grade

| Concept     | Definition                                          | Example                                                  |
| ----------- | --------------------------------------------------- | -------------------------------------------------------- |
| **Quality** | Degree to which deliverable meets requirements      | Software has no critical bugs, meets acceptance criteria |
| **Grade**   | Category assigned based on features/characteristics | Basic vs. Premium versions; economy vs. luxury           |

**Key Insight**: Low quality is always a problem (defects, rework). Low grade may be acceptable if it meets the project's needs (basic version may be sufficient for MVP).

### Prevention vs. Inspection Costs

| Approach                 | Definition                    | Cost Profile              | Examples                                          |
| ------------------------ | ----------------------------- | ------------------------- | ------------------------------------------------- |
| **Prevention**           | Avoid defects from occurring  | High upfront, low ongoing | Training, peer reviews, standards, design reviews |
| **Inspection/Detection** | Find defects after they occur | Low upfront, high ongoing | Testing, audits, inspections, rework              |

**PMP Principle**: **Prevention is preferred over inspection**. It's cheaper to prevent defects than to find and fix them later. This is why agile emphasizes built-in quality (TDD, CI/CD, DoD).

### Cost of Quality (COQ)

| Category                   | Type            | Purpose                     | Examples                                      |
| -------------------------- | --------------- | --------------------------- | --------------------------------------------- |
| **Prevention Costs**       | Conformance     | Prevent defects             | Training, process design, tool investment     |
| **Appraisal Costs**        | Conformance     | Detect defects              | Testing, inspections, audits, reviews         |
| **Internal Failure Costs** | Non-conformance | Fix defects before delivery | Rework, scrap, retest, root cause analysis    |
| **External Failure Costs** | Non-conformance | Fix defects after delivery  | Warranty, support, recalls, reputation damage |

**Exam Insight**: External failure costs are the most expensive because they include customer impact, reputation damage, and potential legal liability.

### 7 Basic Quality Tools in Execution

Use these tools during execution to identify, analyze, and solve quality problems.

| Tool                                             | Purpose                                 | When to Use                                 | Example Application                                                                                  |
| ------------------------------------------------ | --------------------------------------- | ------------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| **Cause-and-Effect Diagram (Fishbone/Ishikawa)** | Identify root causes                    | Major defect or recurring issue             | "Why are customers reporting login failures?" → Categories: People, Process, Technology, Environment |
| **Flowchart**                                    | Map a process to find inefficiencies    | Process improvement, understanding workflow | Map deployment process to find bottlenecks                                                           |
| **Check Sheet**                                  | Collect data systematically             | Tracking defect types, frequencies          | Tally defects by category each sprint                                                                |
| **Pareto Chart**                                 | Identify vital few causes (80/20 rule)  | Prioritizing which issues to fix first      | Top 20% of defect types cause 80% of failures                                                        |
| **Histogram**                                    | Show frequency distribution             | Understanding variation in data             | Distribution of response times                                                                       |
| **Control Chart**                                | Monitor process stability over time     | Continuous monitoring; SPC                  | Track defect rate sprint-over-sprint                                                                 |
| **Scatter Diagram**                              | Show relationship between two variables | Investigating correlations                  | Does code complexity correlate with defect rate?                                                     |

**Control Chart Details:**

- **UCL (Upper Control Limit)**: Maximum acceptable value
- **LCL (Lower Control Limit)**: Minimum acceptable value
- **Mean**: Expected average
- **Rule of 7**: Seven consecutive points on one side of the mean indicates a process shift (assignable cause)

::: tip Exam Application
If a scenario describes recurring quality issues during execution, look for answers that use **root cause analysis** (5 Whys, Fishbone) and **prevention** (process improvement, training, standards) rather than just adding more testing.
:::

### 6. Value Dashboards and Reporting

**Dashboard Design Principles:**

- **Audience-specific**: Executives need summaries; teams need details
- **Visual**: Use charts, traffic lights, trend lines (not just tables)
- **Actionable**: Highlight variances and trends requiring attention
- **Real-time (when possible)**: Automate data feeds to reduce manual effort

**Example: Value Realization Dashboard Structure**

| Section                   | Metrics                                                | Visualization                    |
| ------------------------- | ------------------------------------------------------ | -------------------------------- |
| **Executive Summary**     | Overall value score (green/yellow/red), ROI projection | Scorecard with status indicators |
| **Benefit Realization**   | Benefits on track vs at risk vs realized               | Bar chart by benefit category    |
| **Leading Indicators**    | Adoption rates, training completion, pilot feedback    | Line chart showing trends        |
| **Financial Performance** | Cost actuals vs plan, benefit value to date            | Variance chart                   |
| **Risk to Value**         | Risks impacting benefit realization                    | Risk heat map                    |

---

## Continuous Improvement Processes

Value delivery improves through systematic learning and refinement during execution.

### 1. Kaizen (Continuous Improvement Mindset)

**Kaizen Philosophy**: Small, incremental improvements made continuously by everyone on the team.

**Core Principles:**

- **Everyone contributes**: From team members to stakeholders
- **Small changes**: Low-risk, easy to implement
- **Daily focus**: Improvement is part of everyday work
- **Data-driven**: Measure before and after to confirm improvement

**Kaizen Event (Improvement Workshop):**

1. **Select focus area**: High-waste process or bottleneck
2. **Analyze current state**: Value stream map, process observation
3. **Identify improvements**: Brainstorm solutions, prioritize by impact/effort
4. **Implement changes**: Make improvements immediately
5. **Measure impact**: Track metrics before/after, standardize if successful

**Example: Reducing Code Review Cycle Time**

```
Current State: Code reviews take 2-3 days on average
Root Cause: Reviewers not notified promptly; unclear review checklist
Improvements Implemented:
- Automated Slack notifications when PR is ready
- Created review checklist template
- Set expectation: reviews within 24 hours

Result: Average review time reduced to 8 hours (73% improvement)
```

### 2. Retrospectives (Agile Inspect-and-Adapt)

**Purpose**: Regular team reflection to identify what's working and what needs improvement.

**Retrospective Cadence:**

- **Sprint Retrospectives**: End of each sprint (every 2 weeks)
- **Phase/Release Retrospectives**: End of major milestones
- **Project Post-Mortem**: End of project (lessons learned)

**Retrospective Format (Classic):**

1. **Set the stage** (5 min): Create safe environment, review purpose
2. **Gather data** (10 min): What happened? Facts, metrics, timeline
3. **Generate insights** (15 min): Why did it happen? Patterns, root causes
4. **Decide what to do** (10 min): Actionable improvements for next iteration
5. **Close** (5 min): Appreciate contributions, commit to actions

**Retrospective Techniques:**

| Technique                                     | When to Use                     | Description                                                                           |
| --------------------------------------------- | ------------------------------- | ------------------------------------------------------------------------------------- |
| **Start/Stop/Continue**                       | Simple, quick retros            | What should we start doing, stop doing, and continue doing?                           |
| **Glad/Sad/Mad**                              | Emotional processing needed     | How did the sprint make you feel? What can we do about it?                            |
| **4 Ls** (Liked, Learned, Lacked, Longed For) | Learning-focused                | Encourages reflection on both positive and improvement areas                          |
| **Sailboat**                                  | Visualizing blockers            | What's propelling us forward? What's anchoring us back? What rocks (risks) are ahead? |
| **Timeline**                                  | Complex sprint with many events | Plot events on timeline, identify highs/lows, discuss patterns                        |

**Retrospective Action Items:**

- **Specific and actionable**: "Pair program on complex features" (not "communicate better")
- **Owned**: Assign a person to each action item
- **Tracked**: Review action items at start of next retrospective
- **Limited**: 2-3 actions per retrospective (focus on what matters most)

::: tip PMP Exam Insight
If a scenario describes recurring team issues (missed deadlines, quality problems, conflicts), the answer often involves **implementing retrospectives** or similar reflection practices—not just fixing the immediate problem.
:::

### 3. Lessons Learned (Knowledge Management)

**Purpose**: Capture insights from project experiences to improve future projects.

**When to Capture Lessons Learned:**

- **Throughout the project**: Don't wait until the end
- **After major milestones**: Phase gates, releases, key decisions
- **When issues occur**: Capture lessons while context is fresh
- **Project close**: Comprehensive review

**Lessons Learned Template:**

| Field                | Description                   | Example                                                            |
| -------------------- | ----------------------------- | ------------------------------------------------------------------ |
| **Category**         | Domain of lesson              | Stakeholder management, technical approach, vendor management      |
| **Situation**        | What happened?                | Vendor delivered 3 weeks late, causing critical path delay         |
| **Impact**           | Effect on project             | 3-week schedule slip, $50K cost overrun, stakeholder trust eroded  |
| **Root Cause**       | Why did it happen?            | No penalty clause in contract; vendor over-committed resources     |
| **What Worked**      | Positive aspects to repeat    | Daily standups with vendor improved transparency in later phases   |
| **What Didn't Work** | Issues to avoid               | Relying on verbal commitments without SLA/contract terms           |
| **Recommendation**   | How to apply in future        | Include delivery milestones with penalties in all vendor contracts |
| **Owner**            | Who can implement this lesson | Procurement team, future PMs                                       |

**Making Lessons Learned Actionable:**

- **Store centrally**: PMO knowledge base, searchable repository
- **Review at project initiation**: Check for relevant lessons from similar projects
- **Update templates and processes**: Incorporate lessons into standards
- **Share broadly**: Present at PMO meetings, team trainings

### 4. Plan-Do-Check-Act (PDCA / Deming Cycle)

**PDCA** is a continuous improvement framework for systematic problem-solving.

<div class="pdca-cycle">
 <div class="pdca-step">
 <div class="step-num">1</div>
 <h4>Plan</h4>
 <ul>
 <li>Identify problem or opportunity</li>
 <li>Analyze root cause</li>
 <li>Develop hypothesis and solution</li>
 <li>Define success metrics</li>
 </ul>
 </div>
 <div class="pdca-step">
 <div class="step-num">2</div>
 <h4>Do</h4>
 <ul>
 <li>Implement solution on small scale</li>
 <li>Test hypothesis (pilot, experiment)</li>
 <li>Collect data during implementation</li>
 <li>Document observations</li>
 </ul>
 </div>
 <div class="pdca-step">
 <div class="step-num">3</div>
 <h4>Check</h4>
 <ul>
 <li>Compare results to predictions</li>
 <li>Analyze data and feedback</li>
 <li>Identify gaps and unexpected outcomes</li>
 <li>Determine if solution is effective</li>
 </ul>
 </div>
 <div class="pdca-step">
 <div class="step-num">4</div>
 <h4>Act</h4>
 <ul>
 <li>If successful: standardize and scale</li>
 <li>If unsuccessful: learn and adjust</li>
 <li>Update processes and documentation</li>
 <li>Begin next PDCA cycle for further improvement</li>
 <li>Begin next PDCA cycle for further improvement</li>
 </ul>
 </div>
</div>

**Example: Improving Test Coverage**

```
Plan: Hypothesis - Low test coverage causing production defects.
 Goal: Increase coverage from 60% to 85% in 2 sprints.

Do: Implement automated test generation for new features.
 Train team on test-driven development (TDD).

Check: After 2 sprints, coverage increased to 78%.
 Production defect rate decreased by 40%.

Act: Standardize TDD approach in team guidelines.
 Add coverage checks to CI/CD pipeline.
 Next cycle: Target 90% coverage for critical modules.
```

### 5. Root Cause Analysis (RCA)

When value delivery falls short, identify and address root causes—not just symptoms.

**Common RCA Techniques:**

| Technique                       | Best For                          | Description                                                                         |
| ------------------------------- | --------------------------------- | ----------------------------------------------------------------------------------- |
| **5 Whys**                      | Simple, linear problems           | Ask "Why?" repeatedly (typically 5 times) until root cause is reached               |
| **Fishbone Diagram (Ishikawa)** | Complex, multi-factor problems    | Categorize causes (People, Process, Technology, Materials, Environment, Management) |
| **Pareto Analysis**             | Prioritizing among many causes    | Identify the 20% of causes driving 80% of issues (80/20 rule)                       |
| **Fault Tree Analysis**         | Safety/compliance critical issues | Work backward from failure to identify all contributing factors                     |

**5 Whys Example: Customer Adoption Lower Than Expected**

```
Problem: Only 40% of target users adopted new system (target was 80%)

Why? → Users find the system difficult to use
Why? → Training was inadequate for their needs
Why? → Training content didn't match actual user workflows
Why? → Training was developed without user input
Why? → Project timeline didn't allocate time for user validation

Root Cause: Insufficient user involvement in training design
Action: Involve users in training development for future releases
```

---

## Adaptive vs. Predictive Value Delivery

Different project approaches deliver value in fundamentally different ways.

### Value Delivery Comparison

| Aspect                       | Predictive (Waterfall)                  | Adaptive (Agile)                   | Hybrid                                  |
| ---------------------------- | --------------------------------------- | ---------------------------------- | --------------------------------------- |
| **When value is delivered**  | End of project (big bang)               | Incrementally throughout project   | Phased releases combining both          |
| **Value realization timing** | Delayed until project completes         | Early and continuous               | Varies by component                     |
| **Customer feedback**        | Primarily at gates/reviews              | Continuous throughout execution    | Frequent at iteration boundaries        |
| **Ability to pivot**         | Low (scope defined upfront)             | High (responds to change)          | Moderate (defined release boundaries)   |
| **Risk to value**            | High (all-or-nothing at end)            | Lower (validates incrementally)    | Moderate (phased risk reduction)        |
| **Best for**                 | Fixed requirements, regulatory projects | Uncertain requirements, innovation | Mix of stable and evolving requirements |

### Adaptive Value Delivery Practices

**1. Incremental Value Delivery**

- **Minimum Viable Product (MVP)**: Smallest version that delivers core value
- **Iterative releases**: Each sprint delivers working, potentially shippable product
- **Prioritization by value**: Highest-value features delivered first

**2. Validated Learning**

- **Build-Measure-Learn**: Rapid experimentation to test assumptions
- **Pivot or Persevere**: Based on feedback, adjust approach or continue
- **Fail fast, learn fast**: Prefer small failures early over large failures late

**3. Continuous Delivery**

- **Automated deployment pipelines**: Enable frequent releases
- **Feature flags**: Deploy features but enable selectively
- **Canary releases**: Test with small user subset before full rollout

**Example Scenario:**

```
Predictive Approach: Build entire e-commerce platform over 12 months,
 launch everything at once, measure success at month 13.
 Risk: If users don't adopt, 12 months wasted.

Adaptive Approach: Month 1-2: MVP with product catalog and checkout
 Month 3: Add user accounts and order history
 Month 4: Add product recommendations
 Month 5: Add reviews and ratings
 Each release: Measure adoption, gather feedback, adjust priorities.
 Risk: Mitigated through continuous validation.
```

### Predictive Value Delivery Considerations

**Strengths:**

- **Comprehensive planning**: Detailed requirements, designs, schedules
- **Predictable budgets**: Fixed scope enables accurate cost estimation
- **Regulatory compliance**: Full documentation and traceability
- **Vendor contracts**: Well-defined scope for procurement

**Value Delivery Challenges:**

- **Late validation**: No working product until late in project
- **Change resistance**: Changes are costly and disruptive
- **Benefits delayed**: No value until final delivery

**Mitigation Strategies:**

- **Proof of concepts**: Validate high-risk assumptions early
- **Phased delivery**: Break project into stages with intermediate value
- **Prototypes and mockups**: Get stakeholder feedback before full build
- **User acceptance testing (UAT)**: Validate before final deployment

---

## Measuring Value After Delivery

Benefits may appear after the project is "done." Strong answers ensure:

- A scheduled **benefits review cadence** (post go-live at 30/60/90 days, then quarterly)
- Clear **ownership** (who reports value—typically operational leaders, not PM)
- A feedback loop to refine the product/operations if outcomes lag
- **Benefits realization reports** presented to governance (steering committee, PMO)
- **Comparison of actual vs. planned benefits** with variance analysis and corrective actions

---

## Real-World Value Delivery Scenarios

### Scenario 1: The "Successful" Failure

**Situation**: A project to implement a new CRM system was delivered on time, within budget, and met all documented requirements. However, three months post-implementation, the sales team has reverted to using spreadsheets and the CRM adoption rate is only 25%.

**What went wrong?**

- **Output delivered** (working CRM system)
- **Outcome not achieved** (sales team not using system)
- **Benefit not realized** (no productivity improvement, no data insights)
- **Value missing** (investment wasted, no strategic impact)

**Root Causes:**

1. **Lack of user involvement**: Requirements gathered from management, not actual users
2. **Inadequate change management**: No ADKAR—users lack desire and ability
3. **No validation during execution**: No pilot, no UAT with real users
4. **Training mismatch**: Generic training, not workflow-specific

**PMP Best Practices Missed:**

- Validate outcomes, not just verify outputs
- Engage end users throughout execution (demos, feedback sessions, UAT)
- Implement robust change management (ADKAR)
- Use leading indicators (pilot adoption) to predict lagging indicators (full adoption)

**Corrective Actions:**

- Conduct user listening sessions to understand pain points
- Redesign training based on actual workflows
- Identify and empower user champions
- Implement gradual rollout with support and incentives
- Track adoption metrics and iterate

---

### Scenario 2: Benefit Realization at Risk

**Situation**: A digital transformation project aimed to reduce manual processing time by 50%, saving $500K annually. Six months post-implementation, the new system is operational, but processing time has only improved by 15%, and cost savings are $150K.

**Analysis:**

| Benefit Component         | Target | Actual | Status  |
| ------------------------- | ------ | ------ | ------- |
| Processing time reduction | 50%    | 15%    | At risk |
| Annual cost savings       | $500K  | $150K  | At risk |
| User adoption             | 90%    | 65%    | At risk |

**Root Cause Analysis (5 Whys):**

```
Why are cost savings below target?
→ Processing time reduction is only 15%, not 50%

Why is processing time reduction low?
→ Only 65% of users have adopted the new system

Why is adoption low?
→ Users find the new system slower than manual process for certain tasks

Why is the system slower?
→ System not optimized for high-frequency use cases

Why wasn't this identified earlier?
→ No pilot with real user workflows before full rollout
```

**PMP Corrective Actions:**

1. **Immediate**: Conduct system performance optimization for high-frequency workflows
2. **Short-term**: Launch targeted training for non-adopters, assign user champions
3. **Medium-term**: Implement process improvements identified by users
4. **Governance**: Update benefits register to "at risk," escalate to sponsor
5. **Measurement**: Increase adoption monitoring cadence (weekly vs. monthly)
6. **Communication**: Transparent reporting to stakeholders with recovery plan

**Lessons Learned:**

- Pilot critical workflows with real users before full deployment
- Track leading indicators (adoption, user satisfaction) during transition
- Build flexibility into benefits targets (aggressive/realistic/minimum scenarios)

---

### Scenario 3: Adaptive Value Delivery Success

**Situation**: A mobile app development project was planned as a 12-month waterfall effort with a single release. The PM convinced stakeholders to use an adaptive approach with monthly releases instead.

**Timeline and Value Delivery:**

| Month | Release | Features Delivered                   | Value Realized              | User Feedback                  |
| ----- | ------- | ------------------------------------ | --------------------------- | ------------------------------ |
| 2     | MVP     | Login, product catalog, basic search | 5,000 users signed up       | "Search is too slow"           |
| 3     | v1.1    | Improved search, shopping cart       | 15,000 users, $50K revenue  | "Need saved carts"             |
| 4     | v1.2    | Saved carts, order history           | 30,000 users, $150K revenue | "Want product recommendations" |
| 5     | v1.3    | Recommendations, wishlist            | 50,000 users, $300K revenue | "Great! Need reviews"          |
| 6     | v1.4    | Reviews, ratings, social share       | 75,000 users, $500K revenue | High satisfaction              |

**Key Success Factors:**

1. **Early value delivery**: Revenue started at month 3 (vs. month 13 in waterfall)
2. **Validated learning**: Each release incorporated user feedback
3. **Risk mitigation**: Issues discovered and fixed incrementally
4. **Stakeholder confidence**: Visible progress every month built trust
5. **Market responsiveness**: Competitor launched similar app at month 7, but this project already had 50K users

**Benefits Realized:**

- **Time-to-market**: Value delivered 10 months earlier than waterfall
- **Revenue**: $500K by month 6 (vs. $0 in waterfall at same point)
- **User adoption**: Organic growth through incremental quality improvements
- **Competitive advantage**: First-mover advantage maintained

**PMP Principles Demonstrated:**

- Prioritize by value (MVP first, then enhancements)
- Continuous validation (user feedback drives priorities)
- Incremental delivery reduces risk
- Leading indicators (user growth, engagement) predict benefits

---

### Scenario 4: Stakeholder Resistance During Execution

**Situation**: A regulatory compliance project requires process changes across five departments. Three months into execution, the Operations department is resisting adoption, claiming "the new process is too bureaucratic."

**Symptoms:**

- Operations manager skipping steering committee meetings
- Operations staff not participating in training sessions
- Negative commentary spreading to other departments
- Risk: Project may fail to achieve compliance even if system is delivered

**PMP Response Framework:**

**1. Diagnose the Issue**

- One-on-one meeting with Operations manager (listen, understand concerns)
- Review stakeholder engagement plan—is Operations classified correctly?
- Identify root cause: lack of early involvement? legitimate process issues?

**2. Update Stakeholder Engagement Plan**

| Stakeholder        | Current State       | Desired State      | Strategy                                            |
| ------------------ | ------------------- | ------------------ | --------------------------------------------------- |
| Operations Manager | Resistant           | Supportive         | Increased engagement, address concerns, co-creation |
| Operations Team    | Unaware → Resistant | Aware → Supportive | Involve in process design, demonstrate value        |

**3. Engagement Tactics**

- **Co-creation**: Invite Operations to redesign parts of the process they find "bureaucratic"
- **Pilot program**: Let Operations test refined process before mandate
- **Champions**: Identify supportive Operations staff to advocate for change
- **Transparency**: Share compliance risk if process not adopted (avoid blame)
- **Quick wins**: Identify aspects Operations likes, amplify those

**4. Communication Plan Update**

- Weekly touch-bases with Operations manager
- Monthly lunch-and-learn sessions demonstrating process efficiency
- Share success stories from other departments

**5. Escalation (if needed)**

- If resistance continues, escalate to sponsor for executive alignment
- Frame as risk to compliance (business impact), not personal conflict

**Outcome:**

- Operations co-designs streamlined process (removes 2 unnecessary steps)
- Operations champions identified and empowered
- Adoption improves from 20% to 85% within 6 weeks
- Benefits: Compliance achieved, Operations feels ownership

**PMP Lessons:**

- Resistant stakeholders often have legitimate concerns—listen first
- Engagement is not one-size-fits-all—tailor approach to each stakeholder
- Co-creation drives ownership and adoption
- Never punish or ignore resistance—address it proactively

---

### Scenario 5: Continuous Improvement in Action

**Situation**: An agile team notices their sprint velocity has been declining over the past three sprints (32 → 28 → 24 story points). During the retrospective, they identify root causes and implement improvements.

**Retrospective Findings:**

| What's Working            | What's Not Working                         | Action Items                                          |
| ------------------------- | ------------------------------------------ | ----------------------------------------------------- |
| Team collaboration strong | Too many unplanned interruptions           | Implement "focus time" blocks (no meetings 9-12 AM)   |
| Code quality high         | Testing bottleneck slowing releases        | Pair QA with devs earlier in sprint                   |
| Demos well-received       | Unclear acceptance criteria causing rework | Require AC review in backlog refinement before sprint |

**PDCA Cycle Implementation:**

**Plan**: Hypothesis—Velocity decline caused by interruptions, testing delays, and unclear AC

- **Target**: Restore velocity to 32+ by addressing top 3 issues

**Do**: Implement changes for next sprint

- Focus time blocks (no meetings 9-12 AM daily)
- QA attends daily standup, starts testing mid-sprint
- AC review required before story enters sprint

**Check**: Measure results after 2 sprints

- Velocity: Sprint 7 = 30 points, Sprint 8 = 33 points
- Interruptions: Reduced by 60% during focus time
- Testing: No testing bottleneck; stories completed earlier
- Rework: Decreased by 40% due to clearer AC

**Act**: Standardize improvements

- Make focus time a team norm (added to working agreement)
- QA participation in sprint planning now standard
- Update "Definition of Ready" to require AC review

**Value Impact:**

- **Velocity restored**: Team back to sustainable pace
- **Quality maintained**: No increase in defects despite higher velocity
- **Morale improved**: Team feels empowered and heard
- **Predictability**: More consistent delivery enables better planning

**PMP Lessons:**

- Regular retrospectives identify issues before they become crises
- Small, incremental improvements compound over time
- Team ownership of solutions drives commitment
- Data-driven decisions (velocity trends) justify changes

---

<style>
.chain-grid {
 display: grid;
 grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
 gap: 1.25rem;
 margin: 1.5rem 0;
}

.chain-card {
 padding: 1.25rem;
 background: var(--vp-c-bg-soft);
 border: 1px solid var(--vp-c-border);
 border-radius: 12px;
}

.chain-title {
 font-weight: 700;
 color: var(--vp-c-brand);
 margin-bottom: 0.25rem;
}

.chain-tag {
 font-size: 0.7rem;
 font-weight: 800;
 text-transform: uppercase;
 color: var(--vp-c-text-2);
 margin-bottom: 0.75rem;
}

.chain-card p {
 font-size: 0.85rem;
 margin: 0;
 line-height: 1.4;
}
</style>

---

<div class="study-tip">
 <strong> Exam Insight:</strong> If an exam scenario says a customer is unhappy with a project that met all its technical requirements, the root cause is likely a <strong>failure to focus on Outcomes/Value</strong>. Requirements are a means to an end, not the end itself.
</div>

<style>
.study-tip {
 background: var(--vp-c-brand-soft);
 border-left: 4px solid var(--vp-c-brand);
 padding: 1rem;
 border-radius: 8px;
 margin: 2rem 0;
}

.kano-grid {
 display: grid;
 grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
 gap: 1.25rem;
 margin: 1.5rem 0;
}

.kano-card {
 padding: 1.25rem;
 background: var(--vp-c-bg-soft);
 border: 1px solid var(--vp-c-border);
 border-radius: 12px;
}

.kano-title {
 font-weight: 700;
 color: var(--vp-c-brand);
 margin-bottom: 0.75rem;
 font-size: 1rem;
}

.brm-phases {
 display: grid;
 grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
 gap: 1.5rem;
 margin: 2rem 0;
}

.brm-phase {
 padding: 1.5rem;
 background: var(--vp-c-bg-soft);
 border: 2px solid var(--vp-c-border);
 border-radius: 12px;
 position: relative;
}

.phase-num {
 position: absolute;
 top: -15px;
 left: 20px;
 background: var(--vp-c-brand);
 color: white;
 width: 30px;
 height: 30px;
 border-radius: 50%;
 display: flex;
 align-items: center;
 justify-content: center;
 font-weight: bold;
 font-size: 1rem;
}

.brm-phase h4 {
 margin-top: 0.5rem;
 margin-bottom: 0.5rem;
 color: var(--vp-c-brand);
}

.brm-phase ul {
 margin: 0.75rem 0 0 0;
 padding-left: 1.25rem;
 font-size: 0.9rem;
}

.brm-phase li {
 margin-bottom: 0.35rem;
}

.engagement-techniques {
 display: grid;
 grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
 gap: 1.25rem;
 margin: 1.5rem 0;
}

.technique-card {
 padding: 1.25rem;
 background: var(--vp-c-bg-soft);
 border: 1px solid var(--vp-c-border);
 border-radius: 12px;
}

.technique-card h4 {
 margin-top: 0;
 margin-bottom: 0.75rem;
 color: var(--vp-c-brand);
}

.technique-card p {
 margin: 0.5rem 0;
 font-size: 0.9rem;
 line-height: 1.5;
}

.balanced-scorecard {
 display: grid;
 grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
 gap: 1.25rem;
 margin: 1.5rem 0;
}

.scorecard-quadrant {
 padding: 1.5rem;
 background: var(--vp-c-bg-soft);
 border: 2px solid var(--vp-c-brand-dimm);
 border-radius: 12px;
}

.scorecard-quadrant h4 {
 margin-top: 0;
 margin-bottom: 0.75rem;
 color: var(--vp-c-brand);
 font-size: 1rem;
}

.scorecard-quadrant p {
 margin: 0.5rem 0;
 font-size: 0.85rem;
 line-height: 1.4;
}

.pdca-cycle {
 display: grid;
 grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
 gap: 1.25rem;
 margin: 2rem 0;
}

.pdca-step {
 padding: 1.5rem;
 background: var(--vp-c-bg-soft);
 border: 2px solid var(--vp-c-border);
 border-radius: 12px;
 position: relative;
}

.pdca-step .step-num {
 position: absolute;
 top: -15px;
 left: 20px;
 background: var(--vp-c-brand);
 color: white;
 width: 30px;
 height: 30px;
 border-radius: 50%;
 display: flex;
 align-items: center;
 justify-content: center;
 font-weight: bold;
 font-size: 1rem;
}

.pdca-step h4 {
 margin-top: 0.5rem;
 margin-bottom: 0.75rem;
 color: var(--vp-c-brand);
}

.pdca-step ul {
 margin: 0.5rem 0 0 0;
 padding-left: 1.25rem;
 font-size: 0.85rem;
}

.pdca-step li {
 margin-bottom: 0.35rem;
}
</style>
